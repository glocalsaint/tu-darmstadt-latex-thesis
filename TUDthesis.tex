\documentclass[article,dr=phil,type=drfinal,colorback,accentcolor=tud9c]{tudthesis}
%\usepackage{ngerman}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pdfpages}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{algorithm2e}
\usepackage[noend]{algpseudocode}

% Farbpalette A
\definecolor{blau_1a}{RGB}{93,133,195}
\definecolor{blau_2a}{RGB}{0,156,218}
\definecolor{gruen_3a}{RGB}{80,182,149}
\definecolor{gruen_4a}{RGB}{175,204,80}
\definecolor{gruen_5a}{RGB}{221,223,72}
\definecolor{orange_6a}{RGB}{255,224,92}
\definecolor{orange_7a}{RGB}{248,186,60}
\definecolor{rot_8a}{RGB}{238,122,52}
\definecolor{rot_9a}{RGB}{233,80,62}
\definecolor{lila_10a}{RGB}{201,48,142}
\definecolor{lila_11a}{RGB}{128,69,151}

% Farbpalette B
\definecolor{blau_1b}{RGB}{0,90,169}
\definecolor{blau_2b}{RGB}{0,131,204}
\definecolor{gruen_3b}{RGB}{0,157,129}
\definecolor{gruen_4b}{RGB}{153,192,0}
\definecolor{gruen_5b}{RGB}{201,212,0}
\definecolor{orange_6b}{RGB}{253,202,0}
\definecolor{orange_7b}{RGB}{245,163,0}
\definecolor{rot_8b}{RGB}{236,101,0}
\definecolor{rot_9b}{RGB}{230,0,26}
\definecolor{lila_10b}{RGB}{166,0,132}
\definecolor{lila_11b}{RGB}{114,16,133}
\newcommand{\getmydate}{%
  \ifcase\month%
    \or Januar\or Februar\or M\"arz%
    \or April\or Mai\or Juni\or Juli%
    \or August\or September\or Oktober%
    \or November\or Dezember%
  \fi\ \number\year%
}

\begin{document}
  \thesistitle{Parallel word sense disambigation of Hyperlex Algorithm}%
    {Parallel word sense disambigation of Hyperlex Algorithm}
  \author{Viswanath Vadhri}
  \birthplace{Darmstadt}
  \referee{Prof. Dr. Felix Wolf}{Sebastian Rinke}
  \department{Fachbereich Informatik}
  \group{Laboratory for Parallel Programming}
  \dateofexam{\today}{\today}
  \tuprints{12345}{1234}
  \makethesistitle
  \affidavit{V. Vadhri}

\newpage
\begin{abstract}
Word sense disambiguation (WSD) is the ability to identify the intended meanings of words (word senses) in a context. It is a central research topic in Natural Language Processing (NLP). Word sense disambiguation is often characterized as an intermediate task, which is not an end in itself, but essential for many applications requiring broad-coverage language understanding. Examples include machine translation, information retrieval, information extraction or text mining.

Hyperlex is an unsupervised graph-based technique used in Natural Language Processing that is capable of automatically determining word senses from a large text base without recourse to a dictionary with excellent precision. In this thesis, we design and implement the Hyperlex algorithm as a scalable application which runs on the cluster and can process data of magnitude Terabytes. We analyse the results of the implementation.
\end{abstract}


\newpage
\tableofcontents

\newpage
\chapter{}
  \section{Introduction}
  Word sense disambiguation is about automatically recognizing which of multiple meanings of ambiguous words is being used in a specific sentence. Words have different properties one of them is polysemy. Polysemy means many words have multiple senses. Here is an example 'Lets have a drink in the bar'. The word bar in this sentence means a drinking establishment, however in general the word bar has many other meanings. so for example 'bring me a chocalate bar'. 'I have to study for the bar', the meaning of bar in this case it means some exam for lawyers.

Lets have a drink in the bar
Bring me a chocolate bar.
I have to study for the bar.

Here are a few meanings taken from the wordnet dictionary for the word 'bar'

\begin{enumerate}
  \item barroom, bar, saloon, ginmill, taproom - a room or establishment where alcoholic drinks are served over a counter; "he drowned his sorrows in whiskey at the bar"
  \item bar - a counter where you can obtain food or drink; "he bought a hot dog and a coke at the bar"
  \item prevention, bar - the act of preventing; "there was no bar against leaving"; "money was allocated to study the cause and prevention of influenza"
  \item legal profession, bar, legal community - the body of individuals qualified to practice law in a particular jurisdiction; "he was admitted to the bar in New Jersey"
\end{enumerate}


So the main goal of WSD here is to take a word like bar that has multiple senses and then in a given sentence findout which of those meanings is being used. So we need have the entire context, so we typically get access to the entire sentence and often to the entire paragraph in which the word appears.
    \footnote{Deutschland hat (noch) keine Amtssprache.}

  \section{Application}
    Word sense disambiguation is very important component of Natural Language processing systems because its used for example to come up with for example semantic represntations of sentences and also for question-answering and for things like maching translation. Obviously if a word is ambiguous in one language it doesnt necessarily mean it is also ambiguous in another language. So for example if we want to translate the word 'play' from English to Spanish. We need to understand what is the object being played, so for example in english we say 'play the violin', in spanish this is translated with the verb 'tocar el violin' and then if the sentense is 'play tennis' we are going to use a different verb 'jugar al tennis'. So 'tocar' and 'jugar' are two different translations of the word 'play' and if we couldnt do proper word sense disambiguation in English we wouldnt be able to translate those sentences into spanish properly.

Other uses of WSD include
Accent restoration, for example the word 'cote' in frence means different things when its pronounced differently depending upon where the accents appear
Text-to-speech generation, In this case we may have a word that has multiple pronunciations depending on the meaning for example 'lead'(action) or 'lead'(metal)
Spelling correction, we want to be able to distinguish between words like 'aid' and 'aide'
Capitalization restoration, If we have a word like 'turkey', if we know its meaning as a bird or a country we can properly capitalize in a text in which it is not capitalized.

\newpage
\section{WSD: State of the Art}

\subsection{Lesk Algorithm}
some of the common techniques used in word sense disambiguation starting a very old method by 'Michael Lesk'. It is called dictionary method. The idea behind this method is that if you have ambiguous words in a sentence that appear together, you are going to find all the possible meanings of each of those words and then look up for all the dictionary definitions of all the sense of those words and then look for pairs of dictionary definitions, one for each of the ambiguous words that overlap the most.

For example, lets look at the sentence that has two ambiguous words. 'The leaf is the food making factory for the green plants'. The ambiguous words in this sentence are 'plant' and 'leaf'. These words have many diffferent senses listed in the dictionary, lets look at two.
Definitions of plant:
plant1 : a living thing that grows in the ground, usually has leaves or flowers, and needs sun and water to survive.
plant2 : a building or factory where something is made.
Definintions of leaf:
leaf1 : a lateral outgrowth from the plant stem that is typically a flattened expanded variably shaped greenish organ, function primarily in food manufacture by photosynthesis
leaf2 : a part of the book or a folded sheet that contains a page on each side

So we have the word leaf, the word plant and we want to determine which of those sense is used in the sentence. So we have four possible combinations, leaf1plant1, leaf2plant2, leaf1plant2, leaf2plant1. We are going to look for the overlap in the dictionary definitions and pick the one that has the largest overlapping.
\newpage
\section{Decision Lists}

\newpage
\section{WSD Approaches}
There are majorly two approaches for Word Sense Disambiguation, which
are Supervised learning approach and Unsupervised learning approach.
Knowledge based approach is also another approach which has been implemented
by few older systems. Below are the description given for these approaches.
\subsection{Knowledge Based Approach}
Knowledge based approach for WSD involves use of dictionaries,
thesaurus, ontologism, etc. to understand the sense of words in context. Even
though these methods have comparatively lower performance than other
approaches, but one advantage of this approach is that they do have large-scale
knowledge resources.

\subsection{Supervised Learning Approach}
Supervised learning method is that method which tries to find relationships
between independent variables also known as input attributes and a target
attribute also known as dependent variable. It makes use of labeled training data
to derive functions. These derived functions are used further to predict results.

In supervised approaches, use of training data is involved.
Generating training data manually requires lot of manual efforts plus the data
can’t be trusted on its accuracy. Since the training data does not have the inputs
classified correctly, this can result in getting wrong disambiguated results

\subsection{Unsupervised Learning Approach}
In Unsupervised learning method it tries to find hidden structure in
unlabeled data. Unsupervised methods for WSD can be broadly divided into two
categories namely vector clustering-based and graph based ones. For graph based
methods generally unsupervised approach is preferred since it offers an
advantage of not requiring the training data.
Clustering – finding natural groupings of items. According to Markov clustering algorithm, considering a graph, there will be many links within a cluster, and fewer links between clusters. This means if you were to start at a node, and then randomly travel to a connected node, you’re more likely to stay within a cluster than travel between clusters.


\newpage
\section{Hyperlex}
Hyperlex is an excellent graph based unsupervised approach that is capable of automatically determining word uses in a textbase without recourse to a dictionary. The algorithm makes use of the specific properties of word co-occurance graphs, which are shown as having small-world properties. The small-world nature of a graph can be explained in terms of its clustering coefficient and characteristic path length. The clustering coefficient of a graph shows the extent to which nodes tend to form connected groups that have many edges connecting each other in the group, and few edges leading out of the group. On the other side, the characteristic path length represents “closeness” in a graph. Randomly built graphs exhibit low clustering coefficients and are believed to represent something very close to the minimal possible average path length, at least in expectation. Perfectly ordered graphs, on the other side, show high clustering coefficients but also high average path length. According to Watts and Strogatz (1998), small-world graphs lie between these two extremes: they exhibit high clustering coefficients, but short average path lengths.

HyperLex algorithm accepts a collection of sentences including the target word as an input. Then
it induces the senses of the target words as follows: (1) create a co-occurrence graph whose
nodes are words in the context of the target word, (2) the nodes which
represents the sense, called ‘hub’, are identified, (3) the graph is subdivided into several
sub-trees whose root nodes are the identified hubs, (4) and finally a sense of a word in a new context is disambiguated using the sub-trees. The detail of each step will be described in the
following sections.

\subsection{Building cooccurance graphs}

Hyperlex depends on two most important charecterestics of to extract sense from words. They are frequency and Co-occurance. Frequency is the number of times a word occurs in the text corpora. A target word is said to cooccur with another word if they both appear in the same context. The cooccurance count is the number of times the two words cooccur in the corpora. These two parameters form the basis of calculating weights of the edges of the graph. 

Only nouns and adjectives are considered to be the target words for disambiguation as including verbs causes a notable decline in performance since too many verbs like \textit{can} and \textit{start} have very general uses and they tend to cooccur with lot of words and do not help in the disambiguation process. Words with less than 10 occurances in the entire subcorpus should be discarded. Contexts containing fewer than 4 words after filtering should be deleted and only those cooccurances with a cooccurance count of 5 or should be retained.

A graph G(V,E) is constructed for every target word that is to be disambiguated with all the cooccuring words as the vertices V. The edges of the graph depend on whether the cooccuring words of the target word are cooccuring themselves. 

The edges of the graph are assigned weights which depends on the frequency of the connecting words and the cooccurance count. The weight decreases as the cooccurance count increases. It is calculated as

\textit{\[W_{A,B} =  1 - max[p(A|B),p(B|A)]\]}

where \textit{p(A|B)} denotes the conditional probability of observing A in a given context, knowing that context contains B, and inversely, \textit{p(B|A)} is the probability of observing B in a given context, knowing that it contains A. The probabilities depend on the frequencies of the individual words and the cooccurance count.
\textit{\[p(A|B) = f_{A,B}/f_B,\]}
\textit{\[p(B|A) = f_{B,A}/f_A\]}

Veronis implemented Hyperlex on ten highy polysemous words. Table 2 shows the number of contexts in which the word pairs \textit{eau - ouvrage} (water - work) and \textit{eau - potable}(water - drinkable) appear together or seperately in the \textit{barrage} subcorpus.\newline

\textit{p(eau|ouvrage) =} 183/479 = 0.38,\ \  \textit{p(ouvrage|eau) = } 183/1057 = 0.17,\ \    \textit{w} = 1 \textminus\  0.38 = 0.62.
\newline


\textit{p(eau|potable) = }63/63 = 1, \ \ \ \textit{p(potable|eau) = }63/1057 = 0.06, \ \ \  \textit{w = }1 \textminus\ 1 = 0.

The weight of the edge reflects the magnitude of the semantic distance between words. When the words always cooccur then the weight is equal to 0 and when the words never cooccur then the weight is 1. Edges with weight above 0.9 should be eliminated to make sure that only strong associations are included in the graph. Otherwise the graph tends to become totally connected as the corpus grows in size, due to accidental cooccurances of any word pairs.

 
  \ctable[
   caption = Number of cooccurances of eau-ouvrage(water-work) and eau-potable(water-drinkable),
    cap = Technical Data of the Mitsubishi i-MiEV,
    label   = tab:data_imiev,
    pos = htb,
    mincapwidth = \textwidth,
    width = \textwidth,
    left
    ]{lccc cccccccccccccccccccccccc   ccccrrr}
    {}
    {\FL    &&&&&&&&&  EAU &&&&&&&&& \textasciitilde EAU &&&&&&&&&  Total
        \ML OUVRAGE &&&&&&&&& 183 &&&&&&&&& 296 &&&&&&&&& 489
		\NN \textasciitilde OUVRAGE &&&&&&&&& 874 &&&&&&&&& 5556 &&&&&&&&& 6430        
		\NN &&&&&&&&& &&&&&&&&& &&&&&&&&&
        \NN Total &&&&&&&&&  1057 &&&&&&&&& 5822 &&&&&&&&& 6909
        \NN &&&&&&&&& &&&&&&&&& &&&&&&&&&
        \NN POTABLE &&&&&&&&&  63 &&&&&&&&& 0 &&&&&&&&& 63
        \NN \textasciitilde POTABLE &&&&&&&&&  994 &&&&&&&&& 5852 &&&&&&&&& 6846
        \NN  &&&&&&&&& &&&&&&&&& &&&&&&&&&
        \NN Total &&&&&&&&& 1057 &&&&&&&&& 5852 &&&&&&&&& 6909
        \LL}

\subsection{Detection of root hubs}

\begin{figure}[htb]
	\centering
	\includegraphics[width=150mm]{images/roothubdetection}
	\caption[Step-by-step deletion of neighbours]{Step-by-step deletion of neighbours}
	\label{fig:roothubdetection}
\end{figure}

\begin{algorithm}[H]
	
 \KwIn{\textit{G}: cooccurrence graph; \newline \textit{Freq}: array of frequencies of nodes in G}

 \textit{V} $\leftarrow$ array of nodes in G sorted in decreasing order of frequency\;
 \textit{H} $\leftarrow$ $\emptyset$\;
 \While{ \textit{V} $\neq$ $\emptyset$ \textbf{et} \textit{Freq(V[0])} < \textit{threshold}}{
  \textit{v} $\leftarrow$ \textit{V}[0]\;
  \If{GoodCandidate(\textit{v})}{
   \textit{H} $\leftarrow$ \textit{H} $\cup$ \textit{v}\;
   \textit{V} $\leftarrow$ \textit{V} \textminus \  (\textit{v} $\cup$ \textit{$\Gamma$}(\textit{v}))\;
   }
 }
 \textbf{return} \textit{H}
\end{algorithm}

\subsection{Delineating components}
\begin{figure}[htb]
	\centering
	\includegraphics[]{images/delineate}
	\caption[Minimum spanning tree and high-density components.]{Minimum spanning tree and high-density components.}
	\label{fig:delineate}
\end{figure}


\begin{algorithm}[H]
	
 \KwIn{\textit{G}: cooccurrence graph; \newline \textit{H}: set of root hubs \newline \textit{t}: target word }

 \textit{G'} $\leftarrow$ \textit{G} $\cup$ \textit{t} \;
 
 \ForEach {$h  \in   H $}{
	add edge < \textit{t}, \textit{h} > with weight 0 to \textit{G'} 
 }
 \textit{T} $\leftarrow$ MST(\textit{G'}, \textit{t}) \newline
 \textbf{return} \textit{T}
\end{algorithm}

\subsection{Disambiguation}

\[ \mathbf{s_i} = \frac{1}{1 + \mathit{d}(\mathit{h_{i,v}})} \ \ \textrm{if \textit{v} belongs to component \textit{i},}\] 
\[ \mathbf{s_i} = 0 \ \ \textrm{otherwise.} \]


\newpage
\section{Implementation details}
The implementation of the application is broken down into 5 phases
1. File Processing
2. First level (hashing)
3. Second Level
4. Graph processing
5. Disambiguation
\newpage
\subsection{Input data and conll files}
The application is run on a cluster with varying number of cores and data to get the expermental results for strong scaling weak scaling and we assume that the application is scalable upto terabytes of input data. The input data for the application are conll format[link to conll paper] files which is shown in the figure ... Each sentence is assigned an id. The words in each sentence are annotated with their lemma or base word and their parts of speech.  As mentioned in the hyperlex paper only the adjectives and nouns are considered for disambiguation and all the words which are occuring less than 10 times in the whole data are removed and the ones which are co-occuring less than 5 times are also not considered.

\newpage
\subsection{File Processing}
MPI IO provides simultaneous parallel access to a file by a group of processes.  The basic MPI::File::seek and MPI::File::read function work much like their unix counterparts. They are independent operations that use local file pointers to determine where in the file to access data for each process. While reading the files the processes a chunk of data from the file with broken sentences or incomplete sentences on either side of the data. we fixed this by sending the broken sentences to the corresponding processes. Once the processes have all complete sentences they proceed for processing the data, in which they extract the words along with their parts of speech, their frequency and their co-occuring words.

\newpage
\subsection{Compression and Decompression}
In order to reduce memory latency, the files are read only once and stored in compressed format in the memory. So that they can be retrieved faster and worked upon. The files are compressed by upto 2%.

\newpage
\subsection{First level}
After the initial processing of the data that is read from the files, it so happens that the same words are present in multiple process with partial attribute data like frequency and the co-occuring words. To continue to further processing each word along with all its attribute data must be present in a single process. Initially we broadcasted the words present in each process to every other process and in case the other processes has the same words they sent back the words attribute data and deleted that data in their memory. And this was done linearly from process with rank 0 to n-1. But this resulted in an uneven distribution of data where the first few processes contained large amounts of data and it declined exponentially towards the last process. We bettered this approach by doing in a round robin fashion instead of doing it linearly and by sending only a pre-determined amount of words at a time. But this approach was much inefficient as it increased the amount of communication involved between the processes and thereby increasing the time taken to process.

Keeping in mind, the amount of communication involved between processes and an even distribution of data, we came up with a hash distribution technique where we assign a word to a process depending on their hash value. We calculate the hash of the word string and divides it with MPI communicator size 'n-1' and the remainder is the processes rank which is responsible for this word. Each process can individually calculate the hash for the words they have and send them to the process which is responsible for that word, resulting in a push mechanism where the communication is reduced by half. Fig. ... shows the distribution of the number of words among various processes and it shows that it is almost an even distribution of data.

Figure \ref{fig:mitsubishi_imiev} shows two pictures side by side with one caption.

\begin{figure}[htb]
	\centering
	\includegraphics[width=150mm]{images/12}
	\caption[Hash Distribution]{Hash Distribution}
	\label{fig:hashdistribution}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[page=1,width=80mm]{sequence1.pdf}
\caption[First Level Sequence Diagram]{First Level Sequence Diagram}
	\label{fig:seq1}
\end{figure}

\newpage
\subsection{Second level}
To form a graph for a target word, we need all the co-occuring words(first level words) and also the edges between the co-occuring words. There is an edge between two co-occuring words only if the two words are co-occuring themselves. So we need to get the co-occuring words(second level words) of the co-occuring words of the target word to be able to construct a graph.

\newpage
\subsection{Graph Creation and WSI}
Once we have the data regarding the first level words along with all the edges between these words, we construct the graph and the roothubs are extracted from the graph. The roothubs are the words which are more frequently co-occuring with the target word. These root hubs form the word senses of the target word. Once we extract the root hubs, we perform an MST on the graph to delineate the words(in other words to align each word to its corresponding roothub), and the distance from itself to its corresponding roothub becomes its score which is later used in disambiguation phase.

\newpage
\subsection{Disambiguation}

The 

\begin{figure}[htb] 
    \centering
    \begin{tikzpicture} 
        \begin{axis}[
            ybar,
            xlabel = Month,
            xmin = 0.5,
            xmax = 13.5,
            ymin = 0,
            ymax = 35,
            axis x line* = bottom,
            axis y line* = left,
            ylabel= New electric vehicles,
            width= 0.9\textwidth,
            height = 0.6\textwidth,
            ymajorgrids = true,
            bar width = 5mm,
            xticklabels = \empty,
            extra x ticks = {1,2,3,4,5,6,7,8,9,10,11,12,13},
            extra x tick labels = {Jan '09, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, Jan '10},
            ]
            \addplot+[mark=none, blau_2b, very thick] coordinates {
                (1,3)
                (2,3)
                (3,7)
                (4,1)
                (5,28)
                (6,13)
                (7,4)
                (8,7)
                (9,4)
                (10,14)
                (11,10)
                (12,18)
                (13,35)
            };
        \end{axis} 
    \end{tikzpicture}
    \caption[New electric vehicles between Januar 2010 and Januar 2011]{New electric vehicles between Januar 2010 and Januar 2011 \cite{sa-neuzulassungen}}
    \label{fig:new_ev}
\end{figure}

  
\end{document}
